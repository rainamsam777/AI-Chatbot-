{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4blEa4ol69F"
      },
      "outputs": [],
      "source": [
        "# Step 1: Iotnstall required libraries\n",
        "!pip install tensorflow pandas nltk\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Step 2: Extract and Load the Dataset\n",
        "uploaded_file_path = '/content/archive.zip'  # Path to the uploaded zip file\n",
        "extracted_path = '/content/dataset/'  # Path to extract the zip file\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(uploaded_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "\n",
        "# List files in the extracted folder to identify dataset\n",
        "extracted_files = os.listdir(extracted_path)\n",
        "print(f\"Files in extracted folder: {extracted_files}\")\n",
        "\n",
        "# Automatically detect the dataset file (assuming it is a CSV)\n",
        "dataset_file = None\n",
        "for file in extracted_files:\n",
        "    if file.endswith('.txt'):\n",
        "        dataset_file = os.path.join(extracted_path, file)\n",
        "        break\n",
        "\n",
        "if not dataset_file:\n",
        "    raise FileNotFoundError(\"No txt file found in the extracted folder. Please check the uploaded ZIP file.\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(dataset_file, delimiter='\\t', header=None, names=['input','response'])\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 3: Data Preprocessing\n",
        "# Ensure the dataset has 'input' and 'response' columns\n",
        "if 'input' not in df.columns or 'response' not in df.columns:\n",
        "    raise ValueError(\"Dataset must contain 'input' and 'response' columns.\")\n",
        "\n",
        "# Cleaning the text (lowercasing, tokenizing)\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(str(text).lower())\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['input'] = df['input'].apply(preprocess_text)\n",
        "df['response'] = df['response'].apply(preprocess_text)\n",
        "\n",
        "# Splitting the dataset\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization and vectorization\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(df['input'].tolist() + df['response'].tolist())\n",
        "\n",
        "input_sequences = tokenizer.texts_to_sequences(train_data['input'])\n",
        "response_sequences = tokenizer.texts_to_sequences(train_data['response'])\n",
        "\n",
        "# Padding sequences\n",
        "max_sequence_len = 50\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_len, padding='post')\n",
        "response_sequences = tf.keras.preprocessing.sequence.pad_sequences(response_sequences, maxlen=max_sequence_len, padding='post')\n"
      ]
    }
  ]
}